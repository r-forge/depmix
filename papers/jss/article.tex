\documentclass[article]{jss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Ingmar Visser\\University of Amsterdam \And 
        Maarten Speekenbrink\\University College London }
\title{\pkg{depmixS4}: An \proglang{R}-package for hidden Markov models}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Ingmar Visser, Maarten Speekenbrink} %% comma-separated
\Plaintitle{depmixS4: An R-package for hidden Markov models} %% without formatting
\Shorttitle{depmixS4 Hidden Markov Models} %% a short title (if necessary)

%% an abstract and keywords
	\Abstract{ 
	
	\pkg{depmixS4} implements a general framework for definining and
	fitting hidden Markov mixture model in the R programming language
	\citep{R2009}.  This includes standard Markov models,
	latent/hidden Markov models, and latent class and finite mixture
	distribution models.  The models can be fitted on mixed
	multivariate data with multinomial and/or gaussian distributions.
	Parameters can be estimated subject to general linear constraints.
	Parameter estimation is done through an EM algorithm or by a
	direct optimization approach using the \pkg{Rdonlp2} optimization
	routine when contraints are imposed on the parameters.  A number
	of illustrative examples are included.
	
}

\Keywords{hidden Markov model, dependent mixture model, mixture model, \proglang{R}}

\Plainkeywords{hidden Markov model, dependent mixture model, mixture model,R} %% without formatting

%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Ingmar Visser\\
Department of Psychology\\
University of Amsterdam,
Roetersstraat 15\\
1018 WB, Amsterdam\\
The Netherlands\\
E-mail: \email{i.visser@uva.nl} \\
URL: \url{http://www.ingmar.org/}
}

%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\batchmode

\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[]{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

Markov and latent Markov models are frequently used in the social
sciences, in different areas and applications.  In psychology, they
are used for modelling learning processes, see \citet{Wickens1982},
for an overview, and \citet{Schmittmann2006} for a recent application.
In economics, latent Markov models are commonly used as regime
switching models, see e.g.\ \citet{Kim1994} and \citet{Ghysels1994}.
Further applications include speech recognition \citep{Rabiner1989},
EEG analysis \citep{Rainer2000}, and genetics \citep{Krogh1998}.  In
those latter areas of application, latent Markov models are usually
referred to as hidden Markov models.

The \pkg{depmixS4} package was motivated by the fact that Markov
models are used commonly in the social sciences, but no comprehensive
package was available for fitting such models.  Common programs for
Markovian models include Panmark \citep{Pol1996}, and for latent class
models Latent Gold \citep{Vermunt2003}.  Those programs are lacking a
number of important features, besides not being freely available.
There are currently some packages in R that handle hidden Markov
models but they lack a number of features that we needed in our
research.  In particular, \pkg{depmixS4} was designed to meet the
following goals:
\begin{enumerate}
	
	\item to be able to handle parameter estimates subject to general
	linear (in)equality constraints
	
	\item to be able to fit transition models with covariates, i.e.,
	to have time-dependent transition matrices
	
	\item to be able to include covariates in the prior or initial
	state probabilities of models
	
	\item to allow for easy extensibility, in particular, to be able
	to add new response distributions, both univariate and
	multivariate, and similarly to be able to allow for the addition
	of other transition models, e.g., continuous time observation
	models
	
\end{enumerate}

Although \pkg{depmixS4} is designed to deal with longitudinal or time
series data, for say $T>100$, it can also handle the limit case with
$T=1$.  In those cases, there are no time dependencies between
observed data, and the model reduces to a finite mixture model, or a
latent class model.  Although there are other specialized packages to
deal with mixture data, one specific feature that we needed ourselves
which is to the best of our knowledge not available in other packages
is the possibility to include covariates on the prior probabilities of
class membership.  In the next section, an outline is provided of the
model and the likelihood equations.


\subsection*{Acknowledgements} 

Ingmar Visser was supported by an EC Framework 6 grant, project 516542
(NEST).  Maarten Speekenbrink was supported by the ESRC Centre for
Economic Learning and Social Evolution (ELSE).  Han van der Maas
provided the speed-accuracy data \cite{Maas2005b} and thereby
neccessitated implementing models with time-dependent covariates.
Brenda Jansen provided the balance scale data set \citep{Jansen2001}
which was the perfect opportunity to test the covariates on the prior
model parameters.  The examples in the help files use both of these
data sets.


\section{The dependent mixture model}

The data considered here, has the general form $O_{1}^{1}, \ldots,
O_{1}^{m}$, $O_{2}^{1}, \ldots, O_{2}^{m}$, \ldots, $O_{T}^{1},
\ldots, O_{T}^{m}$ for an $m$-variate time series of length $T$.  As
an example, consider a time series of responses generated by a single
subject in a reaction time experiment.  The data consists of three
variables, reaction time, accuracy and a covariate which is a pay-off
factor which determines the reward for speed and accuracy.  These
variables are measured on 168, 134 and 137 occasions respectively (in
Figure~\ref{fig:speed} the first part of this series is plotted).

\begin{figure}[htbp]
  \begin{center}
	  \scalebox{1.2}{\includegraphics*{graphs/speed1.pdf}}
	  \caption{Response times, accuracy and pay-off values for
	  the first series of responses in dataset \texttt{speed}.}
	  \label{fig:speed}
  \end{center}
\end{figure}

The latent Markov model is commonly associated with data of this type,
albeit usually only multinomial variables are considered.  However,
common estimation procedures, such as those implemented in
\citet{Pol1996} are not suitable for long time series due to underflow
problems.  In contrast, the hidden Markov model is typically only used
for `long' univariate time series.  In the next sections, the
likelihood and estimation procedure for the hidden Markov model is
described, given data of the above form.  These models are called
dependent mixture models because one of the authors (Ingmar Visser)
thought it was time for a new name for these models\footnote{Only
later did I find out that \citet{Leroux1992} already coined the term
dependent mixture models in an application with hidden Markov mixtures
of Poisson count data.}

The dependent mixture model is defined by the following elements:
\begin{enumerate}
	
	\item a set $\vc{S}$ of latent classes or states $S_{i},\, i=1,
	\ldots , n$,
	
	\item matrices $\mat{A}_t$ of transition probabilities $a_{ij,t}$ for
	the transition from state $S_{i}$ to state $S_{j}$ at time $t$,
	
	\item a set $\vc{B}_t$ of observation functions $b_j^k(\cdot)$ that
	provide the conditional probabilities of observations $O_{t}^k$ 
	associated with latent state $S_{j}$,
	
	\item a vector $\pmb{\pi}$ of latent state initial probabilities
	$\pi_{i}$
\end{enumerate}
When transitions are added to the latent class model, it is more
appropriate to refer to the classes as states.  The word class is
rather more associated with a stable trait-like attribute whereas a
state can change over time. 

The dependent mixture model is then given by the following equations: 
\begin{align}
	S_{t} = A S_{t-1}, t=2, \ldots, T \\
	O_{t} = f(O_{t}|S_{t}),
\end{align}
where $S_{t}$ is a sequence of hidden states, $A$ is a transition
matrix, $O_{t}$ is an (possibly multivariate) observation and $f$ is a
density function for $O_{t}$ conditional on the hidden state $S_{t}$.
In the example data above, $f$ could be a gaussian distribution
function for the response time variable, and a Bernouilli for the
accuracy data.  In the models we are considering here, both the
transition probabilities $A$ and the initial state probabilities $\pi$
may depend on covariates as well as the response distributions $f$. 
See for example \citet{Fruhwirth-Schnatter2006} for an overview of 
hidden Markov models with extensions. 


\subsection{Likelihood}

The log-likelihood of hidden Markov models is usually computed by the
so-called forward-backward algorithm \citep{Baum1966,Rabiner1989}, or
rather by the forward part of this algorithm.  \cite{Lystig2002}
changed the forward algorithm in such a way as to allow computing the
gradients of the log-likelihood at the same time.  They start by
rewriting the likelihood as follows (for ease of exposition the
dependence on the model parameters is dropped here):
\begin{equation}
	L_{T} = Pr(\vc{O}_{1}, \ldots, \vc{O}_{T}) = \prod_{t=1}^{T} 
Pr(\vc{O}_{t}|\vc{O}_{1}, 
	\ldots, \vc{O}_{t-1}), 
	\label{condLike}
\end{equation}
where $Pr(\vc{O}_{1}|\vc{O}_{0}):=Pr(\vc{O}_{1})$. Note that for a 
simple, i.e.\ observed, Markov chain these probabilities reduce to 
$Pr(\vc{O}_{t}|\vc{O}_{1},\ldots, 
\vc{O}_{t-1})=Pr(\vc{O}_{t}|\vc{O}_{t-1})$.
The log-likelihood can now be expressed as:
\begin{equation}
	l_{T} = \sum_{t=1}^{T} \log[Pr(\vc{O}_{t}|\vc{O}_{1}, \ldots, 
\vc{O}_{t-1})].
	\label{eq:condLogl}
\end{equation}

To compute the log-likelihood, \cite{Lystig2002} define the following 
(forward) recursion:
\begin{align}
	\phi_{1}(j) &:= Pr(\vc{O}_{1}, S_{1}=j) = \pi_{j} b_{j}(\vc{O}_{1}) 
	\label{eq:fwd1} \\
\begin{split}
	\phi_{t}(j) &:= Pr(\vc{O}_{t}, S_{t}=j|\vc{O}_{1}, \ldots, 
\vc{O}_{t-1}) \\
	&= \sum_{i=1}^{N} [\phi_{t-1}(i)a_{ij}b_{j}(\vc{O}_{t})] \times 
(\Phi_{t-1})^{-1},
	\label{eq:fwdt} 
\end{split} 
\end{align}
where $\Phi_{t}=\sum_{i=1}^{N} \phi_{t}(i)$. Combining 
$\Phi_{t}=Pr(\vc{O}_{t}|\vc{O}_{1}, \ldots, \vc{O}_{t-1})$, and 
equation~(\ref{eq:condLogl}) gives the following expression for the 
log-likelihood:
\begin{equation}
	l_{T} = \sum_{t=1}^{T} \log \Phi_{t}.
	\label{eq:logl}
\end{equation}


\subsection{Parameter estimation}

Parameters are estimated in \pkg{depmixS4} using the EM algorithm or
through the use of a general Newton-Raphson optimizer.  The EM
algorithm however has some drawbacks.  First, it can be slow to
converge towards the end of optimization (although it is usually
faster than direct optimization at the start, so possibly a
combination of EM and direct optimization is fastest).  Second,
applying constraints to parameters can be problematic; in particular,
EM can lead to wrong parameter estimates when applying constraints.
Hence, in \pkg{depmixS4}, EM is used by default in unconstrained
models, but otherwise, direct optimization is done using \pkg{Rdonlp2}
\cite{Tamura2007,Spellucci2002}, because it handles general linear
(in)equality constraints, and optionally also non-linear constraints.




Need some more on EM and how/why it is justified to do separate weighted
fits of the response models and transition and prior models. 

Also mention use of glm, nnet and possibly other packages that we use. 


\section{Using \pkg{depmixS4}}

Two steps are involved in using \pkg{depmixS4} which are illustrated
below with examples:
\begin{enumerate}
	\item model specification with function \code{depmix}
	
	\item  model fitting with function \code{fit}
\end{enumerate}

\subsection{Example data: speed}

Throughout this manual a data set called \code{speed} is used.  It
consists of three time series with three variables: response time,
accuracy, and a covariate Pacc which defines the relative pay-off for
speeded and accurate responding.  The participant in this experiment
switches between fast responding at chance level and relatively slower
responding at a high level of accuracy.  Interesting hypotheses to
test are: is the switching regime symmetric?  Is there evidence for
two states or does one state suffice?  Is the guessing state actually
a guessing state, i.e., is the probability of a correct response at
chance level (0.5)?

\subsection{A simple example}

A dependent mixture model is defined by the number of states, and by
the response distribution functions, and can be created with the
\code{depmix}-function as follows:

\begin{verbatim} 
set.seed(1)
mod <- depmix(rt~1, data=speed, nstates=2, trstart=runif(4))
\end{verbatim}
The \code{depmix} function returns an object of class \code{depmix}
which contains the model specification (and not a fitted model!).
Note also that start values for the transition parameters are provided
in this call using the \code{trstart} argument. The package does not 
provide automatic starting values. 

The so-defined models needs to be \code{fit}ted with the following: 
\begin{verbatim}
fm <- fit(mod)
\end{verbatim}

The \code{fit}-function returns an object of class
\code{depmix.fitted} which extends the \code{depmix} class, adding
convergence information (and information about constraints if these
were applied, see below).  The \code{print} method provides summary
information on convergence, the log likelihood and the AIC and BIC
values.  These statistics may be extracted using \code{logLik},
\code{AIC} and \code{BIC}, respectively.

\begin{verbatim}
> fm 
Convergence info: Log likelihood converged to within tol. 
'log Lik.' -84.34175 (df=7)
AIC:  182.6835 
BIC:  211.275 
\end{verbatim}

The \code{summary} method of \code{fit}ted models provides the parameter
estimates, first for the prior probabilities model, second for the
transition model, and third for the response models.

\begin{verbatim}
> summary(fm)
Initial state probabilties model 
Model of type multinomial, formula: ~1
Coefficients: 
     [,1]      [,2]
[1,]    0 -11.25688
Probalities at zero values of the covariates.
0.999987 1.291797e-05 

Transition model for state (component) 1 
Model of type multinomial, formula: ~1
Coefficients: 
[1]  0.000000 -2.392455
Probalities at zero values of the covariates.
0.9162501 0.08374986 

Transition model for state (component) 2 
Model of type multinomial, formula: ~1
Coefficients: 
[1] 0.000000 2.139255
Probalities at zero values of the covariates.
0.1053396 0.8946604 

Response model(s) for state 1 

Response model for response 1 
Model of type gaussian, formula: rt ~ 1
Coefficients: 
[1] 6.385492
sd  0.2439376 

Response model(s) for state 2 

Response model for response 1 
Model of type gaussian, formula: rt ~ 1
Coefficients: 
[1] 5.511151
sd  0.1926063 
\end{verbatim}


\subsection{Transition parameters}

The transition matrix is parametrized as a list of multinomial
logistic models.  The initial state probabilities are similarly
parametrized as a multinomial logistic model.  Both models use a
baseline category parametrization, meaning that the parameter for the
base category is fixed at zero (see \citet[see][p.\ 267
ff.]{Agresti2002} for multinomial logistic models and various
parameterizations).  The default base category is the first state.
Hence, for example, for a 3-state model, the initial state probability
model would have three parameters of which the first is fixed at zero
and the other two are freely estimated.

Covariates on the transition probabilities can be specified using a
one-sided formula as in the following example:
\begin{verbatim}
set.seed(1)
mod <- depmix(rt~1, data=speed, nstates=2, 
family=gaussian(), transition=~scale(Pacc), 
instart=runif(2))
fm <- fit(mod)
\end{verbatim}

\begin{verbatim}
	Initial state probabilties model 
	Model of type multinomial, formula: ~1
	Coefficients: 
		 [,1]     [,2]
	[1,]    0 10.71779
	Probalities at zero values of the covariates.
	2.214681e-05 0.9999779 

	Transition model for state (component) 1 
	Model of type multinomial, formula: ~scale(Pacc)
	Coefficients: 
		 [,1]       [,2]
	[1,]    0 -0.9215182
	[2,]    0  1.8649734
	Probalities at zero values of the covariates.
	0.7153513 0.2846487 

	Transition model for state (component) 2 
	Model of type multinomial, formula: ~scale(Pacc)
	Coefficients: 
		 [,1]     [,2]
	[1,]    0 2.471442
	[2,]    0 3.570856
	Probalities at zero values of the covariates.
	0.07788458 0.9221154 

	Response model(s) for state 1 

	Response model for response 1 
	Model of type gaussian, formula: rt ~ 1
	Coefficients: 
	[1] 5.512179
	sd  0.1921098 

	Response model(s) for state 2 

	Response model for response 1 
	Model of type gaussian, formula: rt ~ 1
	Coefficients: 
	[1] 6.3885
	sd  0.2402693 

\end{verbatim}

The summary provides all parameters of the model, also the 
(redundant) zeroes for the base-line category in the multinomial model. 
The summary also prints the transition probabilities at the zero value 
of the covariate. Note that scaling of the covariate is useful in this 
regard as it makes interpretation of these intercept probabilities 
easier. 

\subsection{Multivariate data}

Multivariate data can be modelled by providing a list of formulae as 
well as a list of family objects for the distributions of the various 
responses. In above examples we have only used the response times 
which were modelled with the gaussian distribution. The accuracy data 
are in the \code{speed} data are modelled with a multinomial by 
specifying the following: 
\begin{verbatim}
set.seed(1)
mod <- depmix(list(rt~1,corr~1), data=speed, nstates=2, 
	family=list(gaussian(),multinomial()), transition=~scale(Pacc), 
	instart=runif(2))
fm <- fit(mod)
\end{verbatim}
which provides the following fitted model parameters (only the 
response parameters are given here): 
\begin{verbatim}
	> summary(fm)
	\ldots 
	Response model(s) for state 1 

	Response model for response 1 
	Model of type gaussian, formula: rt ~ 1
	Coefficients: 
	[1] 5.52169
	sd  0.2028857 

	Response model for response 2 
	Model of type multinomial, formula: corr ~ 1
	Coefficients: 
		 [,1]      [,2]
	[1,]    0 0.1030554
	Probalities at zero values of the covariates.
	0.4742589 0.5257411 

	Response model(s) for state 2 

	Response model for response 1 
	Model of type gaussian, formula: rt ~ 1
	Coefficients: 
	[1] 6.39369
	sd  0.2373650 

	Response model for response 2 
	Model of type multinomial, formula: corr ~ 1
	Coefficients: 
		 [,1]     [,2]
	[1,]    0 2.245514
	Probalities at zero values of the covariates.
	0.09573715 0.9042629 	
\end{verbatim}
As can be seen, state 1 has fast response times around chance level, 
whereas state 2 corresponds with slower responding at higher accuracy 
levels. 


\subsection{Adding covariates on the prior probabilities}

The \pkg{depmixS4} contains another data set which is used to
illustrate the use of covariates on the prior probabilities of models,
in this case a latent class model. The \code{balance} data consists 
of 4 binary items (correct-incorrect) on a balance scale task 
\citet{Siegler198?}. The data form a subset of the data published in 
\citet{Jansen199?}. 

Similarly to the transition matrix, covariates on the prior 
probabilities of the latent states (or classes in this case), are 
defined by using a one-sided formula: 
\begin{verbatim}
balance$age <- balance$age-5
set.seed(1)
mod <- mix(list(d1~1,d2~1,d3~1,d4~1), data=balance, nstates=2,
        family=list(multinomial(), multinomial(), multinomial(),
		multinomial()), respstart=c(rep(c(0.9,0.1),4),rep(c(0.1,0.9),4)), 
		prior=~age, initdata=balance)
fm <- fit(mod)
\end{verbatim}
Note here that we define a \code{mix} model instead of a \code{depmix}
models as these data form independent observations.

The summary of the \code{fit}ted model gives (only the prior model is 
shown here): 
\begin{verbatim}
> summary(fm)
Mixture probabilities model 
Model of type multinomial, formula: ~age
		[,1]   [,2]
[1,]    0 -2.5182573
[2,]    0  0.5512996
Probalities at zero values of the covariates.
0.9254119 0.07458815 
\end{verbatim}	
Hence at young ages, children have a high probability of incorrect 
responding in class~1, whereas the prior probability for class~2 
increases with age. 


\subsection{Fixing and constraining parameters}

Using package \pkg{Rdonlp2}, parameters may be fitted subject to
general linear (in-)equality constraints.  Constraining and fixing
parameters is done using the \code{conpat} argument to the
\code{depmix.fit}-function, which specifies for each parameter in the
model whether it's fixed (0) or free (1 or higher).  Equality
constraints can be imposed by having two parameters have the same
number in the \code{conpat} vector.  When only fixed values are
required the \code{fixed} argument can be used instead of
\code{conpat}, with zeroes for fixed parameters and other values (ones
e.g.) for non-fixed parameters.  Fitting the models subject to these
constraints is handled by the optimization routine \code{donlp2}.

\paragraph{Parameter numbering} When using the \code{conpat} and
\code{fixed} arguments, complete parameter vectors should be supplied,
i.e., these vectors should have length of the number of parameters of
the model, which can be obtained by calling \code{npar(object)}.
Parameters are numbered in the following order:
\begin{enumerate}
	\item  the prior model parameters
	\item  the parameters for the transition models
	\item  the response model parameters per state (and subsequently
	per response in the case of multivariate time series)
\end{enumerate}

To see the ordering of parameters use the following:
\begin{verbatim}
mod <- setpars(mod, value=1:npar(mod))
mod
\end{verbatim}

To see which parameters are fixed (by default only baseline parameters
in the multinomial logistic models for the transition models and the
initial state probabilities model):
\begin{verbatim}
mod <- setpars(mod, getpars(mod,which="fixed"))
mod
\end{verbatim}

When fitting constraints it is useful to have good starting values 
for the parameters and hence we first fit the following model: 
\begin{verbatim}
trst=c(0.9,0.1,0,0,0.1,0.9,0,0)
mod <- depmix(list(rt~1,corr~1),data=speed,transition=~Pacc,
	nstates=2,family=list(gaussian(),multinomial()),
	trstart=trst,inst=c(.999,0.001))
fm <- fit(mod)
\end{verbatim}
After this, we use the fitted values from this model to constrain the 
regression coefficients on the transition matrix:
\begin{verbatim}
# start with fixed and free parameters
conpat <- c(0,1,rep(c(0,1),4),1,1,0,1,1,1,0,1)
# constrain the beta's on the transition parameters to be equal
conpat[6] <- conpat[10] <- 2
fm <- fit(fm,equal=conpat)
\end{verbatim}
Using \code{summary} on the fitted model shows that the regression 
coefficients are now estimated at the same value of 12.66. 
Note that these arguments provide the possibility for arbitrary 
constraints, also between, e.g., a multinomial regression coefficient 
for the transition matrix and the mean of a gaussian response model. 
Whether such constraints make sense is hence the responsibility of 
the user. 


\section{Extending \pkg{depmixS4}}

The \pkg{depmixS4} package was built with the aim of having the 
possibility of adding new response distributions (and possibly also 
other models for the prior and transition probabilities models). 
Therefore, the EM algorithm simply calls \code{fit} functions from the 
separate response models without knowing what they are. As a 
consequence, adding user-specified response models is straightforward. 
User-defined distributions should extend the \code{response}-class 
and have the following slots:
\begin{enumerate}
	\item y: the response variable
	\item x: the design matrix, possibly only an intercept
	\item paramaters: a named list with the coefficients and possibly 
	other parameters, e.g., the standard deviation in the gaussian 
	response model
	\item fixed: a vector of logicals indicating whether parameters are 
	fixed
	\item npar: numerical indicating the number of parameters of the model
\end{enumerate}

In the \code{speed} data example, it may be more appropriate to model
the response times as an exgaus distribution rather than using the
gaussian.  We can do so as follows, first we define our own
exgaus-class extending the response-class and set it to be a generic:
\begin{verbatim}
	setClass("exgaus", contains="response")
	setGeneric("exgaus", function(y, pstart = NULL, fixed = NULL, ...) 
	standardGeneric("exgaus"))
\end{verbatim}

The so-defined class now needs a number of methods: 
\begin{enumerate}
	\item constructor: function to create instances of the class 
	with starting values
	\item show method: to print the model to the terminal
	\item dens: the function that computes the density of the responses
	\item getpars and setpars: to get and set parameters 
	\item predict: to generate predict'ed values 
	\item fit: function to fit the model using posterior weights (used 
	by the EM algorithm)
\end{enumerate}

Only the constructor function and the fit function are provided here
and the complete code is provided in the helpfile for \code{makeDepmix}. 
The \code{exgaus} example uses the \pkg{gamlss.distr} package for 
computing the \code{dens}ity and for fitting the parameters. 

The constructor function is defined as: 
\begin{verbatim}
setMethod("exgaus", 
	signature(y="ANY"), 
	function(y,pstart=NULL,fixed=NULL, ...) {
		y <- matrix(y,length(y))
		x <- matrix(1)
		parameters <- list()
		npar <- 3
		if(is.null(fixed)) fixed <- as.logical(rep(0,npar))
		if(!is.null(pstart)) {
			if(length(pstart)!=npar) stop("length of 'pstart' must be ",npar)
			parameters$mu <- pstart[1]
			parameters$sigma <- log(pstart[2])
			parameters$nu <- log(pstart[3])
		}
		mod <- new("exgaus",parameters=parameters,fixed=fixed,x=x,y=y,npar=npar)
		mod
	}
)
\end{verbatim}

The \code{fit} function is defined as follows: 
\begin{verbatim}
setMethod("fit","exgaus",
	function(object,w) {
		if(missing(w)) w <- NULL
		y <- object@y
		fit <- gamlss(y~1,weights=w,family=exGAUS(),
			control=gamlss.control(n.cyc=100,trace=FALSE),
			mu.start=object@parameters$mu,
			sigma.start=exp(object@parameters$sigma),
			nu.start=exp(object@parameters$nu))
		pars <- c(fit$mu.coefficients,fit$sigma.coefficients,fit$nu.coefficients)
		object <- setpars(object,pars)
		object
	}
)
The \code{fit} function defines a trivial \code{gamlss} model with 
only an intercept to be estimated and then sets the fitted parameters 
back into their respective slots in the `exgaus' object. See the help 
for \pkg{gamlss.distr} for interpretation of these parameters. 

After defining all the necessary methods for the new response model, 
we can  now define the dependent mixture model using this reponse. 
The function \code{makeDepmix} is added to \pkg{depmixS4} to have 
full control over model specification, and we need it here. 

We first create all the response models that we want as a double list: 
\begin{verbatim}
rModels <- list(
	list(
		exgaus(rt,pstart=c(5,.1,.1)),
		GLMresponse(formula=corr~1,data=speed,family=multinomial(),pstart=c(0.5,0.5))
	),
	list(
		exgaus(rt,pstart=c(6,.1,.1)),
		GLMresponse(formula=corr~1,data=speed,family=multinomial(),pstart=c(.1,.9))
	)
)
\end{verbatim}
Next, we define the transition and prior probability models using the 
transInit function (which produces a transInit model, which also extends 
the response class): 
\begin{verbatim}
trstart=c(0.9,0.1,0.1,0.9)
transition <- list()
transition[[1]] <- transInit(~Pacc,nstates=2,data=speed,pstart=c(0.9,0.1,0,0))
transition[[2]] <- transInit(~Pacc,nstates=2,data=speed,pstart=c(0.1,0.9,0,0))
inMod <- transInit(~1,ns=2,pstart=c(0.1,0.9),data=data.frame(1))
\end{verbatim}
Finally, we put everything together using \code{makeDepmix} and fit 
the model: 
\begin{verbatim}
mod <- makeDepmix(response=rModels,transition=transition,
prior=inMod,stat=FALSE)
fm <- fit(mod)
\end{verbatim}

Using \code{summary} will print the fitted parameters. Note that the 
use of \code{makeDepmix} allows the possibility of, say, fitting a 
gaussian in one state and an exgaus distribution in another state. 



\section{Conclusion \& future work}


What are our next plans?

Adding gradients for speed and computation of the Hessian. 


\bibliography{all,ingmar}


\end{document}
