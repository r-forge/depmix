%&LaTeX
\documentclass[article]{jss}
\usepackage{amsmath}

%\usepackage[]{amsmath, amsfonts, amstext, amsthm} 
%\usepackage{amssymb}
%\usepackage[]{graphics} 
%\usepackage{epsfig}
%\usepackage{epstopdf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Ingmar Visser\\University of Amsterdam \And 
        Maarten Speekenbrink\\University College London}
        
\title{\pkg{depmixS4} : An \proglang{R}-package for hidden Markov models}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Ingmar Visser, Maarten Speekenbrink} %% comma-separated
\Plaintitle{depmixS4: An R-package for hidden Markov models} %% without formatting

\Shorttitle{depmixS4: Hidden Markov Models} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ 	

	\pkg{depmixS4} implements a general framework for defining and estimating
	 dependent mixture models in the \proglang{R} programming language
	\citep{R2009}. This includes standard Markov models, latent/hidden Markov
	models, and latent class and finite mixture distribution models. The models can
	be fitted on mixed multivariate data with distributions from the \code{glm} 
	family, the logistic multinomial, or the multivariate normal distribution. Other 
	distributions can be added easily, and an example is provided. Parameter 
	estimation is done through an EM algorithm or by a direct optimization approach using
	the\pkg{Rdonlp2} optimization routine when contraints are imposed on the parameters. 
	Parameters can be estimated subject to general linear constraints.  
}

\Keywords{hidden Markov model, dependent mixture model, mixture model}

\Plainkeywords{hidden markov model, dependent mixture model, mixture model} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{13}
%% \Issue{9}
%% \Month{September}
%% \Year{2004}
%% \Submitdate{2004-09-29}
%% \Acceptdate{2004-09-29}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
Ingmar Visser\\
Department of Psychology\\
University of Amsterdam\\
Roetersstraat 15\\
1018 WB, Amsterdam\\
The Netherlands\\
E-mail: \email{i.visser@uva.nl} \\
URL: \url{http://www.ingmar.org/}
}

%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/1/31336-5053
%% Fax: +43/1/31336-734

\newcommand{\vc}{\mathbf}
\newcommand{\mat}{\mathbf}
\newcommand{\greekv}[1]{\mbox{\boldmath$\mathrm{#1}$}}
\newcommand{\greekm}[1]{\mbox{\boldmath$#1$}}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\batchmode

\begin{document}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

% Refs to check: Jansen 2002, Dutilh 2009


\section{Introduction}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

Markov and latent Markov models are frequently used in the social
sciences, in different areas and applications.  In psychology, they
are used for modelling learning processes, see \citet{Wickens1982},
for an overview, and \citet{Schmittmann2006} for a recent application.
In economics, latent Markov models are commonly used as regime
switching models, see e.g.\ \citet{Kim1994} and \citet{Ghysels1994}.
Further applications include speech recognition \citep{Rabiner1989},
EEG analysis \citep{Rainer2000}, and genetics \citep{Krogh1998}.  In
those latter areas of application, latent Markov models are usually
referred to as hidden Markov models. See e.g.\ \citet[][chapter~1]{Cappe2005}
for an overview of different applications. 

The \pkg{depmixS4} package was motivated by the fact that Markov
models are used commonly in the social sciences, but no comprehensive
package was available for fitting such models.  Common programs for
Markovian models include Panmark \citep{Pol1996}, and for latent class
models Latent Gold \citep{Vermunt2003}.  Those programs are lacking a
number of important features, besides not being freely available.
There are currently some packages in R that handle hidden Markov
models but they lack a number of features that we needed in our
research.  In particular, \pkg{depmixS4} was designed to meet the
following goals:

\begin{enumerate}
	
	\item to be able to handle parameter estimates subject to general
	linear (in)equality constraints
	
	\item to be able to fit transition models with covariates, i.e.,
	to have time-dependent transition matrices
	
	\item to be able to include covariates in the prior or initial
	state probabilities of models
	
	\item to allow for easy extensibility, in particular, to be able
	to add new response distributions, both univariate and
	multivariate, and similarly to be able to allow for the addition
	of other transition models, e.g., continuous time observation
	models
	
\end{enumerate}

Although \pkg{depmixS4} is designed to deal with longitudinal or time
series data, for say $T>100$, it can also handle the limit case with
$T=1$.  In those cases, there are no time dependencies between
observed data, and the model reduces to a finite mixture model, or a
latent class model.  Although there are other specialized packages to
deal with mixture data, one specific feature that we needed ourselves
which is to the best of our knowledge not available in other packages
is the possibility to include covariates on the prior probabilities of
class membership and the transition probabilities. In the next section, 
an outline is provided of the model and the likelihood equations.


\section{The dependent mixture model}

The data considered here, has the general form $O_{1}^{1}, \ldots,
O_{1}^{m}$, $O_{2}^{1}, \ldots, O_{2}^{m}$, \ldots, $O_{T}^{1},
\ldots, O_{T}^{m}$ for an $m$-variate time series of length $T$.  As
an example, consider a time series of responses generated by a single
participant in a response time experiment.  The data consists of three
variables, response time, accuracy and a covariate which is a pay-off
determining the relative reward for speeded and/or accuracy responding. These
variables are measured on 168, 134 and 137 occasions respectively (in
Figure~\ref{fig:speed} the first part of this series is plotted). These data
are more fully described in \citet{Dutilh2009}. 

\begin{figure}[htbp]
  \begin{center}
	  \scalebox{1.2}{\includegraphics*{graphs/speed1.pdf}}
	  \caption{Response times (rt), accuracy (corr) and pay-off values (Pacc) for
	  the first series of responses in dataset \code{speed}.}
	  \label{fig:speed}
  \end{center}
\end{figure}

The latent Markov model is commonly associated with data of this type,
although usually only multinomial response variables are considered.  However,
common estimation procedures, such as those implemented in
\citet{Pol1996} are not suitable for long time series due to underflow
problems.  In contrast, the hidden Markov model is typically only used
for `long' univariate time series 
\cite[see e.g.][, chapter~1 for an overview of examples]{Cappe2005}.  
In the next sections, the
likelihood and estimation procedures for the dependent mixture model is
described for data of the above form.  These models are called
dependent mixture models because one of the authors (Ingmar Visser)
thought it was time for a new name for these models\footnote{Only
later did I find out that \citet{Leroux1992} already coined the term
dependent mixture models in an application with hidden Markov mixtures
of Poisson count data.}.

The dependent mixture model is defined by the following elements:
\begin{enumerate}
	
	\item a set $\vc{S}$ of latent classes or states $S_{i},\, i=1,
	\ldots , n$,
	
	\item matrices $\mat{A}$ of transition probabilities $a_{ij}$ for
	the transition from state $S_{i}$ to state $S_{j}$,
	
	\item a set $\vc{B}$ of observation density functions $b_j^k(\cdot)$ that
	provide the conditional densities of observations $O_{t}^k$ 
	associated with latent state $S_{j}$,
	
	\item a vector $\pmb{\pi}$ of latent state initial probabilities
	$\pi_{i}$
\end{enumerate}

The dependent mixture model is defined by the following equations: 
\begin{align}
	S_{t} &= A S_{t-1}, t=2, \ldots, T \\
	O_{t} &= b(O_{t}|S_{t}), 
\end{align}
where $S_{t}$ is a sequence of hidden states, $A$ is a transition
matrix, $O_{t}$ is an (possibly multivariate) observation and $b$ is a
density function for $O_{t}$ conditional on the hidden state $S_{t}$.
In the example data above, $b$ could be a gaussian distribution
function for the response time variable, and a Bernouilli for the
accuracy data.  In the models we are considering here, both the
transition probabilities $\mat{A}$ and the initial state probabilities $\pi$
may depend on covariates as well as the response distributions $\vc{B}$. 
See for example \citet{Fruhwirth2006} for an overview of 
hidden Markov models with extensions. 

\subsection{Likelihood}

The log-likelihood of hidden Markov models is usually computed by the
so-called forward-backward algorithm \citep{Baum1966,Rabiner1989}, or
rather by the forward part of this algorithm.  \cite{Lystig2002}
changed the forward algorithm in such a way as to allow computing the
gradients of the log-likelihood at the same time.  They start by
rewriting the likelihood as follows (for ease of exposition the
dependence on the model parameters is dropped here):
\begin{equation}
	L_{T} = Pr(\vc{O}_{1}, \ldots, \vc{O}_{T}) = \prod_{t=1}^{T} 
Pr(\vc{O}_{t}|\vc{O}_{1}, 
	\ldots, \vc{O}_{t-1}), 
	\label{condLike}
\end{equation}
where $Pr(\vc{O}_{1}|\vc{O}_{0}):=Pr(\vc{O}_{1})$. Note that for a 
simple, i.e.\ observed, Markov chain these probabilities reduce to 
$Pr(\vc{O}_{t}|\vc{O}_{1},\ldots, 
\vc{O}_{t-1})=Pr(\vc{O}_{t}|\vc{O}_{t-1})$.
The log-likelihood can now be expressed as:
\begin{equation}
	l_{T} = \sum_{t=1}^{T} \log[Pr(\vc{O}_{t}|\vc{O}_{1}, \ldots, 
\vc{O}_{t-1})].
	\label{eq:condLogl}
\end{equation}

To compute the log-likelihood, \cite{Lystig2002} define the following 
(forward) recursion:
\begin{align}
	\phi_{1}(j) &:= Pr(\vc{O}_{1}, S_{1}=j) = \pi_{j} b_{j}(\vc{O}_{1}) 
	\label{eq:fwd1} \\
\begin{split}
	\phi_{t}(j) &:= Pr(\vc{O}_{t}, S_{t}=j|\vc{O}_{1}, \ldots, 
\vc{O}_{t-1}) \\
	&= \sum_{i=1}^{N} [\phi_{t-1}(i)a_{ij}b_{j}(\vc{O}_{t})] \times 
(\Phi_{t-1})^{-1},
	\label{eq:fwdt} 
\end{split} 
\end{align}
where $\Phi_{t}=\sum_{i=1}^{N} \phi_{t}(i)$. Combining 
$\Phi_{t}=Pr(\vc{O}_{t}|\vc{O}_{1}, \ldots, \vc{O}_{t-1})$, and 
equation~(\ref{eq:condLogl}) gives the following expression for the 
log-likelihood:
\begin{equation}
	l_{T} = \sum_{t=1}^{T} \log \Phi_{t}.
	\label{eq:logl}
\end{equation}


\subsection{Parameter estimation}

Parameters are estimated in \pkg{depmixS4} using the EM algorithm or
through the use of a general Newton-Raphson optimizer.  In the EM algorithm, 
parameters are estimated by iteratively maximising the 
expected joint likelihood of the parameters given the observations and states. 
Let $\greekv{\theta} = (\greekv{\theta}_1, \greekv{\theta}_2,\greekv{\theta}_3)$
be the general parameter vector consisting of three subvectors with parameters 
for the prior model, transition model, and response model respectively. The 
joint log likelihood can be written as
\begin{equation}
\log \Prob(O_{1:T}, S_{1:T}|\greekv{\theta}) = \log \Prob(S_1|\greekv{\theta}_1) 
+ \sum_{t=2}^{T} \log \Prob(S_t|S_{t-1},\greekv{\theta}_2) 
+ \sum_{t=1}^{T} \log \Prob(O_t|S_t,\greekv{\theta}_3)
\end{equation}
This likelihood depends on the unobserved states $S_t$. In the Expectation step,
we replace these with their expected values given a set of (initial) parameters 
$\greekv{\theta}' = (\greekv{\theta}'_1, \greekv{\theta}'_2,\greekv{\theta}'_3)$
and observations $O_{1:T}$. The expected log likelihood 
\begin{equation}
Q(\greekv{\theta},\greekv{\theta}') = E_{\greekv{\theta}'} 
(\log \Prob(O_{1:T},S_{1:T}|O_{1:T},\greekv{\theta}))
\end{equation}
can be written as
%\begin{equation}
\begin{multline}
\label{eq:Q}
Q(\greekv{\theta},\greekv{\theta}') = 
\sum_{j=1}^n \gamma_1(j) \log \Prob(S_1=j|\greekv{\theta}_1) \\ 
+ \sum_{t=2}^T \sum_{j=1}^M \sum_{k=1}^n \xi_t^i(j,k) \log \Prob(S_t = k|S_{t-1} 
= j,\greekv{\theta}_2) \\ + \sum_{t=1}^T \sum_{j=1}^n \gamma_t^i(j) 
\ln \Prob(O_t|S_t=j,\greekv{\theta}_3),
\end{multline}
%\end{equation}
where the expected values $\xi_t(j,k) =  P(S_t = k, S_{t-1} = j|O_{1:T},\greekv{\theta}')$ and $\gamma_t(j) = P(S_t = j|O_{1:T},\greekv{\theta}')$ can be computed effectively by the Forward-Backward algorithm \citep[see e.g.,][]{Rabiner1989}. The Maximisation step consists of the maximisation of (\ref{eq:Q}) for $\greekv{\theta}$. As the r.h.s. of (\ref{eq:Q}) consists of three separate parts, we can maximise separately for $\greekv{\theta}_1$, $\greekv{\theta}_2$ and $\greekv{\theta}_3$. In common models, maximisation for $\greekv{\theta}_1$ and $\greekv{\theta}_2$ is performed by the \code{nnet.default} routine in \pkg{nnet}, and maximisation for $\greekv{\theta}_3$ by the \code{glm} routine. 



%The maximising values of $a_{jk}$ are \cite<e.g.,>{Rabiner89}
%\begin{equation}
%\label{eq:MLA}
%\hat{a}_{jk} = \frac{1}{N(T-1)} \sum_{i=1}^N \sum_{t=2}^T  \frac{\xi^i_t(j,k)}{\gamma_{t-1}^i(j)}.
%\end{equation}
%Fixing certain elements of $\mat{A}$ to 0, as in the DT version, does not affect the estimation of the other elements. When elements $a_{jk}$ are assumed identical, we simply extend the summation in (\ref{eq:MLA}) to include all those elements, changing the denominator $N(T-1)$ accordingly.

%To estimate $\greekv{\lambda}$, we note that the term containing this parameter has the form of a weighted likelihood, where $\gamma_t^i(j)$ can be interpreted as the number of replications of $r^i_t$. Hence, we can rely on standard ML estimates of the logistic regression coefficients $\lambda_i$, using the values $\gamma_t^i(j)$ as ``case weights'' \cite<e.g.,>{Agresti02,McCullagh83}\footnote{To be more specific, we replicate each observation $r^i_t$ a total of 6 times, once for each of the states besides the random state (which offers no information regarding $\lambda_i$). For the $j$-th replication (corresponding to the $j$-th state), we used $v_j(\vec{x}_t)$ as a predictor variable and $\gamma^i_t(j)$ as a case weight. All these replications were used to obtain the maximum likelihood estimate of $\lambda$ from a single GLM, using the ``glm.fit'' function in R \cite{R}.}. The resulting parameter estimates are then used in a new Expectation step, followed by another Maximisation step, until convergence. Under mild regularity conditions, it can be shown that the EM algorithm converges to a (local) maximum of the likelihood \cite{EM}. 


The EM algorithm however has some drawbacks.  First, it can be slow to
converge towards the end of optimization.  Second,
applying constraints to parameters can be problematic; in particular,
EM can lead to wrong parameter estimates when applying constraints.
Hence, in \pkg{depmixS4}, EM is used by default in unconstrained
models, but otherwise, direct optimization is done using \pkg{Rdonlp2}
\cite{Tamura2009,Spellucci2002}, because it handles general linear
(in)equality constraints, and optionally also non-linear constraints.

%Need some more on EM and how/why it is justified to do separate weighted
%iuuuuuuuuuuucvfits of the response models and transition and prior models. 

%Also mention use of glm, nnet and possibly other packages that we use. 


\section[Using depmixS4]{Using \pkg{depmixS4}}

Two steps are involved in using \pkg{depmixS4} which are illustrated
below with examples:
\begin{enumerate}
	\item model specification with function \code{depmix} (or with \code{mix}
	for latent class and finite mixture models, see example below on adding
	covariates to prior probabilities)
	
	\item  model fitting with function \code{fit}
\end{enumerate}
We have separated the stages of model specification and model fitting because
fitting large models can be fairly time-consuming and it is hence useful to be
able to check the model specification before actually fitting the model. 

\subsection{Example data: speed}

Throughout this manual a data set called \code{speed} is used.  It
consists of three time series with three variables: response time,
accuracy, and a covariate Pacc which defines the relative pay-off for
speeded and accurate responding.  The participant in this experiment
switches between fast responding at chance level and relatively slower
responding at a high level of accuracy.  Interesting hypotheses to
test are: is the switching regime symmetric?  Is there evidence for
two states or does one state suffice?  Is the guessing state actually
a guessing state, i.e., is the probability of a correct response at
chance level (0.5)?

\subsection{A simple model}

A dependent mixture model is defined by the number of states, and by
the response distribution functions, and can be created with the
\code{depmix}-function as follows:

\begin{CodeChunk}
\begin{CodeInput}
> set.seed(1)
> mod <- depmix(rt~1, data=speed, nstates=2, trstart=runif(4))
\end{CodeInput}
\end{CodeChunk}

The \code{depmix} function returns an object of class \code{depmix}
which contains the model specification (and not a fitted model!).
Note also that start values for the transition parameters are provided
in this call using the \code{trstart} argument. The package does not 
provide automatic starting values. 

The so-defined models needs to be \code{fit}ted with the following
line of code:
\begin{CodeChunk}
\begin{CodeInput}
> fm <- fit(mod)
\end{CodeInput}
\end{CodeChunk}

The \code{fit}-function returns an object of class
\code{depmix.fitted} which extends the \code{depmix} class, adding
convergence information (and information about constraints if these
were applied, see below).  The \code{print} method provides summary
information on convergence, the log likelihood and the AIC and BIC
values:
\begin{CodeChunk}
\begin{CodeInput}
> fm
\end{CodeInput}
\begin{CodeOutput}
Convergence info: Log likelihood converged to within tol. 
'log Lik.' -84.34175 (df=7)
AIC:  182.6835 
BIC:  211.275 
\end{CodeOutput}
\end{CodeChunk}
These statistics may be extracted using \code{logLik},
\code{AIC} and \code{BIC}, respectively.

The \code{summary} method of \code{fit}ted models provides the parameter
estimates, first for the prior probabilities model, second for the
transition model, and third for the response models.

\begin{CodeChunk}
\begin{CodeInput}
> summary(fm)
\end{CodeInput}
\begin{CodeOutput}
Initial state probabilties model 
Model of type multinomial, formula: ~1
Coefficients: 
     [,1]      [,2]
[1,]    0 -11.25688
Probalities at zero values of the covariates.
0.999987 1.291798e-05 

Transition model for state (component) 1 
Model of type multinomial, formula: ~1
Coefficients: 
[1]  0.000000 -2.392455
Probalities at zero values of the covariates.
0.9162501 0.08374986 

Transition model for state (component) 2 
Model of type multinomial, formula: ~1
Coefficients: 
[1] 0.000000 2.139255
Probalities at zero values of the covariates.
0.1053396 0.8946604 

Response model(s) for state 1 

Response model for response 1 
Model of type gaussian, formula: rt ~ 1
Coefficients: 
[1] 6.385492
sd  0.2439376 

Response model(s) for state 2 

Response model for response 1 
Model of type gaussian, formula: rt ~ 1
Coefficients: 
[1] 5.511151
sd  0.1926063 
\end{CodeOutput}
\end{CodeChunk}


\subsection{Covariates on transition parameters}

The transition probabilities and the initial state probabilities
are parameterized using the multinomial
logistic model.   In particular, each row of the transition matrix is
parameterized by a baseline category logistic multinomial, 
meaning that the parameter for the
base category is fixed at zero (see \citet[see][p.\ 267
ff.]{Agresti2002} for multinomial logistic models and various
parameterizations). See also \citet{Chung2007} for similar models, latent transition models using logistic regression on the transition parameters. They fit such models on repeated measurement
data ($T=2$) using Bayesian methods.  The default baseline category is the first state.
Hence, for example, for a 3-state model, the initial state probability
model would have three parameters of which the first is fixed at zero
and the other two are freely estimated.

Covariates on the transition probabilities can be specified using a
one-sided formula as in the following example:
\begin{CodeChunk}
\begin{CodeInput}
set.seed(1)
mod <- depmix(rt~1, data=speed, nstates=2, family=gaussian(), 
  transition=~scale(Pacc), instart=runif(2))
fm <- fit(mod)
\end{CodeInput}
\end{CodeChunk}

Applying \code{summary} to the \code{fit}ted object gives (only transition models
printed here): 
\begin{CodeChunk}
\begin{CodeOutput}
...
Transition model for state (component) 1 
Model of type multinomial, formula: ~scale(Pacc)
Coefficients: 
	 [,1]       [,2]
[1,]    0 -0.9215182
[2,]    0  1.8649734
Probalities at zero values of the covariates.
0.7153513 0.2846487 

Transition model for state (component) 2 
Model of type multinomial, formula: ~scale(Pacc)
Coefficients: 
	 [,1]     [,2]
[1,]    0 2.471442
[2,]    0 3.570856
Probalities at zero values of the covariates.
0.07788458 0.9221154
...
\end{CodeOutput}
\end{CodeChunk}
The summary provides all parameters of the model, also the 
(redundant) zeroes for the base-line category in the multinomial model. 
The summary also prints the transition probabilities
at the zero value of the covariate. Note that scaling of the covariate 
is useful in this regard as it makes interpretation of these intercept probabilities 
easier. 

\subsection{Multivariate data}

Multivariate data can be modelled by providing a list of formulae as 
well as a list of family objects for the distributions of the various 
responses. In above examples we have only used the response times 
which were modelled with the gaussian distribution. The accuracy data 
are in the \code{speed} data are modelled with a multinomial by 
specifying the following: 
\begin{verbatim}
set.seed(1)
mod <- depmix(list(rt~1,corr~1), data=speed, nstates=2, 
	family=list(gaussian(),multinomial()),
	 transition=~scale(Pacc),instart=runif(2))
fm <- fit(mod)
\end{verbatim}
which provides the following fitted model parameters (only the 
response parameters are given here): 
\begin{verbatim}
	> summary(fm)
	...
	Response model(s) for state 1 

	Response model for response 1 
	Model of type gaussian, formula: rt ~ 1
	Coefficients: 
	[1] 5.52169
	sd  0.2028857 

	Response model for response 2 
	Model of type multinomial, formula: corr ~ 1
	Coefficients: 
		 [,1]      [,2]
	[1,]    0 0.1030554
	Probalities at zero values of the covariates.
	0.4742589 0.5257411 

	Response model(s) for state 2 

	Response model for response 1 
	Model of type gaussian, formula: rt ~ 1
	Coefficients: 
	[1] 6.39369
	sd  0.2373650 

	Response model for response 2 
	Model of type multinomial, formula: corr ~ 1
	Coefficients: 
		 [,1]     [,2]
	[1,]    0 2.245514
	Probalities at zero values of the covariates.
	0.09573715 0.9042629 	
\end{verbatim}
As can be seen, state 1 has fast response times around chance level, 
whereas state 2 corresponds with slower responding at higher accuracy 
levels. 


\subsection{Adding covariates on the prior probabilities}

To illustrate the use of covariates on the prior probabilities we have included
another data set with \pkg{depmixS4}. The \code{balance} data consists 
of 4 binary items (correct-incorrect) on a balance scale task 
\citet{Siegler1981}. The data form a subset of the data published in 
\citet{Jansen2002}. 

Similarly to the transition matrix, covariates on the prior 
probabilities of the latent states (or classes in this case), are 
defined by using a one-sided formula: 
\begin{verbatim}
balance$age <- balance$age-5
set.seed(1)
mod <- mix(list(d1~1,d2~1,d3~1,d4~1), data=balance, nstates=2,
        family=list(multinomial(), multinomial(), multinomial(),
		multinomial()), respstart=c(rep(c(0.9,0.1),4),rep(c(0.1,0.9),4)), 
		prior=~age, initdata=balance)
fm <- fit(mod)
\end{verbatim}
Note here that we define a \code{mix} model instead of a \code{depmix}
models as these data form independent observations.

The summary of the \code{fit}ted model gives (only the prior model is 
shown here): 
\begin{verbatim}
> summary(fm)
Mixture probabilities model 
Model of type multinomial, formula: ~age
		[,1]   [,2]
[1,]    0 -2.5182573
[2,]    0  0.5512996
Probalities at zero values of the covariates.
0.9254119 0.07458815 
...
\end{verbatim}	
Hence at young ages, children have a high probability of incorrect 
responding in class~1, whereas the prior probability for class~2 
increases with age. 


\subsection{Fixing and constraining parameters}

Using package \pkg{Rdonlp2} by \citet{Tamura2009}, parameters may be fitted subject to
general linear (in-)equality constraints.  Constraining and fixing
parameters is done using the \code{conpat} argument to the
\code{fit}-function, which specifies for each parameter in the
model whether it's fixed (0) or free (1 or higher).  Equality
constraints can be imposed by having two parameters have the same
number in the \code{conpat} vector.  When only fixed values are
required, the \code{fixed} argument can be used instead of
\code{conpat}, with zeroes for fixed parameters and other values (ones
e.g.) for non-fixed parameters.  Fitting the models subject to these
constraints is handled by the optimization routine \code{donlp2}.
To be able to construct the \code{conpat} and/or \code{fixed} vectors 
one needs the correct ordering of parameters which is briefly discussed 
next before proceeding with an example. 

\paragraph{Parameter numbering} When using the \code{conpat} and
\code{fixed} arguments, complete parameter vectors should be supplied,
i.e., these vectors should have length of the number of parameters of
the model, which can be obtained by calling \code{npar(object)}. Note that
this is not the same as the degrees of freedom used e.g.\ in the \code{logLik}
function because \code{npar} also counts the baseline category zeroes
from the multinomial logistic models. 
Parameters are numbered in the following order:
\begin{enumerate}
	\item  the prior model parameters
	\item  the parameters for the transition models
	\item  the response model parameters per state (and subsequently
	per response in the case of multivariate time series)
\end{enumerate}

To see the ordering of parameters use the following:
\begin{verbatim}
mod <- setpars(mod, value=1:npar(mod))
mod
\end{verbatim}

To see which parameters are fixed (by default only baseline parameters
in the multinomial logistic models for the transition models and the
initial state probabilities model):
\begin{verbatim}
mod <- setpars(mod, getpars(mod,which="fixed"))
mod
\end{verbatim}

When fitting constraints it is useful to have good starting values 
for the parameters and hence we first fit the following model without
constraints:
\begin{verbatim}
trst=c(0.9,0.1,0,0,0.1,0.9,0,0)
mod <- depmix(list(rt~1,corr~1),data=speed,transition=~Pacc,
	nstates=2,family=list(gaussian(),multinomial()),
	trstart=trst,inst=c(.999,0.001))
fm1 <- fit(mod)
\end{verbatim}
After this, we use the fitted values from this model to constrain the 
regression coefficients on the transition matrix (parameters numbers~6 and~10):
\begin{verbatim}
# start with fixed and free parameters
conpat <- c(0,1,rep(c(0,1),4),1,1,0,1,1,1,0,1)
# constrain the beta's on the transition parameters to be equal
conpat[6] <- conpat[10] <- 2
fm2 <- fit(fm,equal=conpat)
\end{verbatim}
Using \code{summary} on the fitted model shows that the regression 
coefficients are now estimated at the same value of 12.66. The function
\code{llratio} computes the likelihood ratio $\chi^2$-statistic and the
associated $p$-value with appropriate degrees of freedom for testing the
tenability of constraints \citep{Dannemann2007}. Note that these arguments 
provide the possibility for arbitrary 
constraints, also between, e.g., a multinomial regression coefficient 
for the transition matrix and the mean of a gaussian response model. 
Whether such constraints make sense is hence the responsibility of 
the user. 


\section[Extending depmixS4]{Extending \pkg{depmixS4}}

The \pkg{depmixS4} package was built with the aim of having the 
possibility of adding new response distributions (and possibly also 
other models for the prior and transition probabilities models). 
Therefore, the EM algorithm simply calls \code{fit} functions from the 
separate response models without knowing what they are. As a 
consequence, adding user-specified response models is straightforward. 
User-defined distributions should extend the \code{response}-class 
and have the following slots:
\begin{enumerate}
	\item y: the response variable
	\item x: the design matrix, possibly only an intercept
	\item paramaters: a named list with the coefficients and possibly 
	other parameters, e.g., the standard deviation in the gaussian 
	response model
	\item fixed: a vector of logicals indicating whether parameters are 
	fixed
	\item npar: numerical indicating the number of parameters of the model
\end{enumerate}

In the \code{speed} data example, it may be more appropriate to model
the response times as an exgaus distribution rather than using the
gaussian.  We can do so as follows, first we define our own
exgaus-class extending the response-class:
\begin{verbatim}
	setClass("exgaus", contains="response")
\end{verbatim}

The so-defined class now needs a number of methods: 
\begin{enumerate}
	\item constructor: function to create instances of the class 
	with starting values
	\item show method: to print the model to the terminal
	\item dens: the function that computes the density of the responses
	\item getpars and setpars: to get and set parameters 
	\item predict: to generate predict'ed values 
	\item fit: function to fit the model using posterior weights (used 
	by the EM algorithm)
\end{enumerate}

Only the constructor function and the fit function are provided here
and the complete code is provided in the helpfile for \code{makeDepmix}. 
The \code{exgaus} example uses the \pkg{gamlss} and \pkg{gamlss.distr} packages \cite{Stasinopoulos2009a,Stasinopoulos2009b} for 
computing the \code{dens}ity and for \code{fit}ting the parameters. 

The constructor function is defined as: 
\begin{verbatim}
setGeneric("exgaus", function(y, pstart = NULL, fixed = NULL, ...) 
	standardGeneric("exgaus"))
setMethod("exgaus", 
	signature(y="ANY"), 
	function(y,pstart=NULL,fixed=NULL, ...) {
		y <- matrix(y,length(y))
		x <- matrix(1)
		parameters <- list()
		npar <- 3
		if(is.null(fixed)) fixed <- as.logical(rep(0,npar))
		if(!is.null(pstart)) {
			if(length(pstart)!=npar) stop("length of 'pstart' must be ",npar)
			parameters$mu <- pstart[1]
			parameters$sigma <- log(pstart[2])
			parameters$nu <- log(pstart[3])
		}
		mod <- new("exgaus",parameters=parameters,fixed=fixed,x=x,y=y,npar=npar)
		mod
	}
)
\end{verbatim}

The \code{fit} function is defined as follows: 
\begin{verbatim}
setMethod("fit","exgaus",
	function(object,w) {
		if(missing(w)) w <- NULL
		y <- object@y
		fit <- gamlss(y~1,weights=w,family=exGAUS(),
			control=gamlss.control(n.cyc=100,trace=FALSE),
			mu.start=object@parameters$mu,
			sigma.start=exp(object@parameters$sigma),
			nu.start=exp(object@parameters$nu))
		pars <- c(fit$mu.coefficients,fit$sigma.coefficients,fit$nu.coefficients)
		object <- setpars(object,pars)
		object
	}
)
\end{verbatim}

The \code{fit} function defines a \code{gamlss} model with 
only an intercept to be estimated and then sets the fitted parameters 
back into their respective slots in the `exgaus' object. See the help 
for \code{gamlss.distr} for interpretation of these parameters. 

After defining all the necessary methods for the new response model, 
we can  now define the dependent mixture model using this reponse. 
The function \code{makeDepmix} is added to \pkg{depmixS4} to have 
full control over model specification, and we need it here. 

We first create all the response models that we want as a double list: 
\begin{verbatim}
rModels <- list(
	list(
		exgaus(rt,pstart=c(5,.1,.1)),
		GLMresponse(formula=corr~1,data=speed,family=multinomial(),pstart=c(0.5,0.5))
	),
	list(
		exgaus(rt,pstart=c(6,.1,.1)),
		GLMresponse(formula=corr~1,data=speed,family=multinomial(),pstart=c(.1,.9))
	)
)
\end{verbatim}
Next, we define the transition and prior probability models using the 
transInit function (which produces a transInit model, which also extends 
the response class): 
\begin{verbatim}
trstart=c(0.9,0.1,0.1,0.9)
transition <- list()
transition[[1]] <- transInit(~Pacc,nstates=2,data=speed,pstart=c(0.9,0.1,0,0))
transition[[2]] <- transInit(~Pacc,nstates=2,data=speed,pstart=c(0.1,0.9,0,0))
inMod <- transInit(~1,ns=2,pstart=c(0.1,0.9),data=data.frame(1))
\end{verbatim}
Finally, we put everything together using \code{makeDepmix} and fit 
the model: 
\begin{verbatim}
mod <- makeDepmix(response=rModels,transition=transition,
prior=inMod,stat=FALSE)
fm <- fit(mod)
\end{verbatim}

Using \code{summary} will print the fitted parameters. Note that the 
use of \code{makeDepmix} allows the possibility of, say, fitting a 
gaussian in one state and an exgaus distribution in another state. 


\section[Conclusions and future work]{Conclusions \& future work}

\pkg{depmixS4} provides a flexible framework for fitting dependent mixture models
for a large variety of response distributions. It can also fit latent class regression
and finite mixture models, although it should be noted that more specialized packages 
are available for doing so, e.g.\ \pkg{FlexMix} \cite{Leisch2004}. The package is intended
for modeling of (individual) time series data with the aim of characterizing the transition 
processes underlying the data. The possibility to use covariates on the transition matrix
greatly enhances the flexibility of the model. The EM algorithm uses a very general 
interface that allows easy addition of new response models. 

We are currently working on implementing the gradients for response and transition models 
with two goals in mind. First, to speed up (constrained) parameter optimization using 
\pkg{Rdonlp2}. Second, analytic gradients are useful in computing the Hessian of the
estimated parameters so as to arrive at standard errors for those. We are also planning to
implement goodness-of-fit statistics \citep{Titman2008}. 


\section*{Acknowledgements} 

Ingmar Visser was supported by an EC Framework 6 grant, project 516542
(NEST).  Maarten Speekenbrink was supported by the ESRC Centre for
Economic Learning and Social Evolution (ELSE).  Han van der Maas
provided the speed-accuracy data \cite{Dutilh2009} and thereby
neccessitated implementing models with time-dependent covariates.
Brenda Jansen provided the balance scale data set \citep{Jansen2002}
which was the perfect opportunity to test the covariates on the prior
model parameters.  The examples in the help files use both of these
data sets.

\bibliography{all,ingmar}


\end{document}
